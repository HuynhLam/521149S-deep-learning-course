{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Deep convolutional GAN using Wasserstein loss</font>\n",
    "#### <font color='green'>Experiment with the MNIST dataset</font>\n",
    "#### <font color='green'>521149S Deep Learning, Oulu 2018</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "%matplotlib inline\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Conv2DTranspose\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from utils import save_images_to_disk, write_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Parameters for training</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'optimizer': 'adam', 'lr': 0.0001, 'loss': 'wasserstein', \n",
    "          'logging': './logging/wdcgan_mnist_adam_bz_32.log', \n",
    "          'images': './gen_images_wdcgan_mnist_adam_bz_32',\n",
    "          'tb_path': './tb_graph/wdcgan_mnist_adam_bz_32',\n",
    "         'epochs': 50001, 'batch_size': 32, 'save_interval': 100}\n",
    "\n",
    "# params = {'optimizer': 'rmsprop', 'lr': 0.00005, 'loss': 'wasserstein', \n",
    "#           'logging': './logging/wdcgan_mnist_rmsprop_bz_32.log', \n",
    "#           'images': './gen_images_wdcgan_mnist_rmsprop_bz_32',\n",
    "#           'tb_path': './tb_graph/wdcgan_mnist_rmsprop_bz_32',\n",
    "#          'epochs': 50001, 'batch_size': 32, 'save_interval': 100}\n",
    "\n",
    "# params = {'optimizer': 'adam', 'lr': 0.0001, 'loss': 'normal', \n",
    "#           'logging': './logging/dcgan_mnist_adam_bz_32.log', \n",
    "#           'images': './gen_images_dcgan_mnist_adam_bz_32',\n",
    "#           'tb_path': './tb_graph/dcgan_mnist_adam_bz_32',\n",
    "#          'epochs': 50001, 'batch_size': 32, 'save_interval': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Create logger</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config logging, set up logging to file\n",
    "# set up logging to file - see previous section for more details\n",
    "if not os.path.exists(os.path.dirname(params['logging'])):\n",
    "    os.makedirs(os.path.dirname(params['logging']))\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n",
    "                    datefmt='%m-%d %H:%M',\n",
    "                    filename=params['logging'],\n",
    "                    filemode='w')\n",
    "# define a Handler which writes INFO messages or higher to the sys.stderr\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "# set a format which is simpler for console use\n",
    "formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')\n",
    "# tell the handler to use this format\n",
    "console.setFormatter(formatter)\n",
    "# add the handler to the root logger\n",
    "logging.getLogger('').addHandler(console)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Building the deep convolutional GAN using Wasserstein loss</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        \n",
    "        if params['optimizer'] == 'adam':\n",
    "            optimizer = Adam(lr=params['lr'], beta_1=0.5, beta_2=0.9)\n",
    "            \n",
    "        elif params['optimizer'] == 'rmsprop':\n",
    "            optimizer = RMSprop(lr=params['lr'])\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Using wrong optimizer!')\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()        \n",
    "        if params['loss'] == 'wasserstein':\n",
    "            self.discriminator.compile(loss=self.wasserstein_loss,\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "        elif params['loss'] == 'normal':\n",
    "            self.discriminator.compile(loss='binary_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "        else:\n",
    "            raise ValueError('Using wrong loss!')\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "\n",
    "        if params['loss'] == 'wasserstein':\n",
    "            self.combined.compile(loss=self.wasserstein_loss, optimizer=optimizer)            \n",
    "            \n",
    "        elif params['loss'] == 'normal':\n",
    "            self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Using wrong loss!')\n",
    "            \n",
    "        \n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        '''Calculates the Wasserstein loss for a sample batch.\n",
    "        The Wasserstein loss function is very simple to calculate. In a standard GAN, the discriminator\n",
    "        has a sigmoid output, representing the probability that samples are real or generated. In Wasserstein\n",
    "        GANs, however, the output is linear with no activation function! Instead of being constrained to [0, 1],\n",
    "        the discriminator wants to make the distance between its output for real and generated samples as large as possible.\n",
    "        The most natural way to achieve this is to label generated samples -1 and real samples 1, instead of the\n",
    "        0 and 1 used in normal GANs, so that multiplying the outputs by the labels will give you the loss immediately.\n",
    "        Note that the nature of this loss means that it can be (and frequently will be) less than 0.\n",
    "        '''\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        \n",
    "#         model.add(UpSampling2D())\n",
    "#         model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        \n",
    "        model.add(Conv2DTranspose(filters=256, kernel_size=3, padding='same', strides=(2,2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "#         model.add(UpSampling2D())\n",
    "#         model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        \n",
    "        model.add(Conv2DTranspose(filters=128, kernel_size=3, padding='same', strides=(1,1)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(Conv2DTranspose(filters=64, kernel_size=3, padding='same', strides=(2,2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(Conv2DTranspose(filters=32, kernel_size=3, padding='same', strides=(1,1)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary(print_fn=logging.info)\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary(print_fn=logging.info)\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        if params['loss'] == 'wasserstein':\n",
    "            fake = np.ones((batch_size, 1), dtype=np.float32)\n",
    "            valid = -fake\n",
    "            \n",
    "        elif params['loss'] == 'normal':\n",
    "            valid = np.ones((batch_size, 1), dtype=np.float32)\n",
    "            fake = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Using wrong loss!')\n",
    "            \n",
    "        # Create tensorboard callback\n",
    "        callback = TensorBoard(log_dir=params['tb_path'],\n",
    "                                histogram_freq=0,\n",
    "                                write_graph=True,\n",
    "                                write_images=False)\n",
    "        callback.set_model(self.generator)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # ***** Train the discriminator *****\n",
    "            # Select a random half of images\n",
    "            idx = np.random.choice(X_train.shape[0], batch_size, replace=False)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ***** Train the generator *****\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "            \n",
    "            # Tensorboard logs\n",
    "            log_names = ['d_loss', 'd_acc', 'g_loss']\n",
    "            log_values = [d_loss[0], 100*d_loss[1], g_loss]\n",
    "            \n",
    "            if epoch % (save_interval // 2) == 0:\n",
    "#                 print(d_loss, d_loss_real, d_loss_fake, g_loss)\n",
    "                write_log(callback, log_names, log_values, epoch)\n",
    "                \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                save_images_to_disk(self, epoch, params['images'])\n",
    "                logging.info('{} [D loss: {}, acc.: {:05f}] [G loss: {}]'.format(epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Main function</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    dcgan.train(epochs=params['epochs'], batch_size=params['batch_size'], save_interval=params['save_interval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
